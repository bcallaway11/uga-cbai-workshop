---
title: "Difference-in-Differences Workshop"
subtitle: "Session 2: Using Covariates in Difference-in-Differences Identification Strategies"
format: clean-revealjs
author:
  - name: Brantly Callaway
    email: brantly.callaway@uga.edu
    affiliations: University of Georgia
knitr:
  opts_chunk:
    echo: true
bibliography:
  - refs.bib
  - labor_refs.bib
---

```{r echo=FALSE}
library(revealEquations)
```


## Additional Resources

[Additional Workshop Materials:]{.alert} [https://bcallaway11.github.io/uga-cbai-workshop/](https://bcallaway11.github.io/uga-cbai-workshop/)


  * Slides, code, data, etc. <!--for the workshop-->

. . .


[General References:]{.alert}

  * @callaway-2023, *Handbook of Labor, Human Resources and Population Economics*

  * Baker, Callaway, Cunningham, Goodman-Bacon, Sant'Anna (2024), draft posted very soon

. . .

[Specific References:]{.alert}

  * @caetano-callaway-2023 for interpreting TWFE regressions with covariates

  * @callaway-santanna-2021 and @santanna-zhao-2020 for alternative estimators

  * Caetano, Callaway, Payne, and Sant'Anna (2022) for "bad controls"


## Review of Session 1

1. Difference-in-Differences with Two Periods and Two Groups

2. Extensions to Staggered Treatment Adoption

    * Limitations of TWFE Regressions
    * Introduction to newer approaches

3. Application about Effects of Minimum Wage Policies on Employment

$\newcommand{\E}{\mathbb{E}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\Var}{\mathrm{var}}
\newcommand{\Cov}{\mathrm{cov}}
\newcommand{\Corr}{\mathrm{corr}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\L}{\mathrm{L}}
\renewcommand{\P}{\mathrm{P}}
\newcommand{\independent}{{\perp\!\!\!\perp}}
\newcommand{\indicator}[1]{ \mathbf{1}\{#1\} }$


## Covariates in the Parallel Trends Assumption

Including covariates in the parallel trends assumption can often make DID identification strategies more plausible:

. . .

[Examples]{.alert-blue}

* [Minimum wage example:]{.alert} path of teen employment may depend on a county's population / population growth / region of the country

* [Job displacement example:]{.alert} path of earnings may depend on year's of education / race / occupation

. . .

However, there are a number of new issues that can arise in this setting...

## Plan: Covariates in the Parallel Trends Assumption

<br>

1. Identification with Two Periods

2. Limitations of TWFE Regressions

3. Alternative Estimation Strategies

4. Multiple Periods and Variation in Treatment Timing

5. Minimum Wage Application

6. Dealing with "Bad" Controls


## Notation

[Data:]{.alert}

* 2 periods: $t=1$, $t=2$
    * No one treated until period $t=2$
    * Some units remain untreated in period $t=2$

* $D_{it}$ treatment indicator in period $t$

* 2 groups: $G_i=1$ or $G_i=0$ (treated and untreated)

. . .

<span class="alert">Potential Outcomes: </span> $Y_{it}(1)$ and $Y_{it}(0)$

. . .

<span class="alert">Observed Outcomes: </span> $Y_{it=2}$ and $Y_{it=1}$

\begin{align*}
  Y_{it=2} = G_i Y_{it=2}(1) +(1-G_i)Y_{it=2}(0) \quad \textrm{and} \quad Y_{it=1} = Y_{it=1}(0)
\end{align*}


## Target Parameter

<span class="alert">Average Treatment Effect on the Treated: </span>
$$ATT = \E[Y_{t=2}(1) - Y_{t=2}(0) | G=1]$$

Explanation: Mean difference between treated and untreated potential outcomes in the second period among the treated group

. . .

Pushing the expectation through the difference, we have that:
\begin{align*}
  ATT = \underbrace{\E[Y_{t=2}(1) | G=1]}_{\textrm{Easy}} - \underbrace{\E[Y_{t=2}(0) | G=1]}_{\textrm{Hard}}
\end{align*}

## Review: Unconditional Parallel Trends

::: {.callout-note}

### (Unconditional) Parallel Trends Assumption


$$\E[\Delta Y(0) | G=1] = \E[\Delta Y(0) | G=0]$$
:::

. . .

[In words:]{.alert} The path of untreated potential outcomes is the same for the treated group as for the untreated group

. . .

[From last time:]{.alert} Under (unconditional) PTA,

$$
\begin{aligned}
  ATT &= \E[\Delta Y | G=1] - \E[\Delta Y(0) | G=1] \\
  &= \E[\Delta Y | G=1] - \E[\Delta Y | G=0]
\end{aligned}
$$


# Part 1: Identification with Two Periods

## Additional Notation for Covariates

Start with the case with only two time periods

More notation about covariates:

* $X_{it=2}$ and $X_{it=1}$ &mdash; time-varying covariates

* $Z_i$ &mdash; time-invariant covariates


## What We'll Do in this Part

[Two identification results:]{.alert}

1. Direct identifcation strategy:

    * Recover $ATT$ using DiD identification strategy at all values of the covariates, and then averaging them

2. Indirect identification strategy:

    * Based on re-weighting the untreated group to make it have the same distribution of covariates as the treated group

. . .

These different identification strategies will suggest alternative ways to estimate $ATT$, which we will return to later...


## Covariates in the Parallel Trends Assumption

<br>

::: {.callout-note}

### Conditional Parallel Trends Assumption


$$\E[\Delta Y(0) | X_{t=2}, X_{t=1},Z,G=1] = \E[\Delta Y(0) | X_{t=2}, X_{t=1},Z,G=0]$$
:::

::: {.fragment}

[In words:]{.alert} Parallel trends holds conditional on having the same covariates $(X_{t=2},X_{t=1},Z)$.

:::

::: {.fragment}

[Minimum wage example]{.alert} (e.g.) Parallel trends conditional on counties have the same population (like $X_{t}$) and being in the same region of the country (like $Z$)

:::

```{r echo=FALSE, results="asis"}

title <- "Identification under Conditional Parallel Trends, Strategy 1"

before <- "Under conditional parallel trends, we have that"

eqlist <- list("ATT &= \\E[\\Delta Y | G=1] - \\E[\\Delta Y(0) | G=1] \\hspace{150pt}",
               "&=\\E\\Big[ \\E[\\Delta Y | X,G=1] \\Big| G=1\\Big] - \\E\\Big[ \\E[\\Delta Y(0) | X, G=1] \\Big| G=1\\Big]",
               "&=\\E\\Big[ \\underbrace{\\E[\\Delta Y | X,G=1] - \\E[\\Delta Y(0) | X, G=0]}_{=:ATT(X)} \\Big| G=1\\Big]")


after <- "

. . .

[Intuition:]{.alert}

1. Compare path of outcomes for treated group to (conditional on covariates) path of outcomes for untreated group, $\\rightarrow$ $ATT(X)$

2. Average $ATT(X)$ to get $ATT$

"

step_by_step_eq(eqlist, before, after, title)
```

## Identification under Conditional Parallel Trends, Strategy 1

From the last slide, we had that

$$ ATT = \E\Big[ \underbrace{\E[\Delta Y | X,G=1] - \E[\Delta Y(0) | X, G=0]}_{=:ATT(X)} \Big| G=1\Big] $$

. . .

For estimation, it is useful to simplify the previous expression to be

$$ATT = \E[\Delta Y | G=1] - \E\Big[ \underbrace{\E[\Delta Y(0) | X, G=0]}_{=:m_0(X)} \Big| G=1\Big] $$

. . .

This expression highlights that the "challenging" term to estimate here will be $m_0(X)$...

* but we will return to this issue later

. . .

The argument above also requires an [overlap condition]{.alert}

* For all possible values of the covariates, $p(x) := \P(G=1|X=x) < 1$.

* In words: for all treated units, we can find untreated units that have the same characteristics


```{r echo=FALSE, results="asis"}

title <- "Identification Strategy 2: Covariate Balancing"

before <- "[Example:]{.alert} Momentarily, suppose that the distribution of $X$ was the same for both groups, then

. . .

"

eqlist <- list("ATT &= \\E[\\Delta Y | G=1] - \\E[\\Delta Y(0) | G=1] \\hspace{150pt}",
  "&= \\E[\\Delta Y | G=1] - \\E\\Big[ \\E[\\Delta Y(0) | X, G=0 ] \\Big| G=1\\Big]",
  "&= \\E[\\Delta Y | G=1] - \\E\\Big[ \\E[\\Delta Y(0) | X, G=0 ] \\Big| G=0\\Big]",
  "&= \\E[\\Delta Y | G=1] - \\E[\\Delta Y(0) | G=0]")


after <- "

. . .

$\\implies$ (even under conditional parallel trends) we can recover $ATT$ by just directly comparing paths of outcomes for treated and untreated groups."

step_by_step_eq(eqlist, before, after, title)
```

```{r echo=FALSE, results="asis"}

title <- "Alternative Identification Strategy: Covariate Balancing"

before <- "[More generally:]{.alert} We would not expect the distribution of covariates to be the same across groups.

. . .

However the idea of covariate balancing is to come up with [balancing weights]{.alert} $\\nu_0(X)$ such that the distribution of $X$ is the same in the untreated group as it is in the treated group after applying the balancing weights.  Then we would have that

. . .

"

eqlist <- list("ATT &= \\E[\\Delta Y | G=1] - \\E[\\Delta Y(0) | G=1] \\hspace{150pt}",
  "&= \\E[\\Delta Y | G=1] - \\E\\Big[ \\E[\\Delta Y(0) | X, G=0 ] \\Big| G=1\\Big]",
  "&= \\E[\\Delta Y | G=1] - \\E\\Big[ \\nu_0(X) \\E[\\Delta Y(0) | X, G=0 ] \\Big| G=0\\Big]",
  "&= \\E[\\Delta Y | G=1] - \\E[\\nu_0(X) \\Delta Y(0) | G=0]")


after <- "

. . .

$\\implies$ We can recover $ATT$ by re-weighting the untreated group to have the same distribution of covariates as the treated group has...and then just average"

step_by_step_eq(eqlist, before, after, title)
```

## Discussion

The arguments about suggest that, in order to estimate the $ATT$, we will either need to

1. Correctly model $\E[\Delta Y(0) | X, G=0]$ (i.e, specifiy a model for $m_0(X)$)

2. Balance the distribution of $X$ to be the same for the untreated group relative to the treated group.

. . .

We will pursue these approaches soon, but next we will switch to considering TWFE regressions that include covariates

# Part 2: Limitations of TWFE Regressions {visibility="uncounted"}

## Limitations of TWFE Regressions

In this setting, it is common to run the following TWFE regression:

$$Y_{it} = \theta_t + \eta_i + \alpha D_{it} + X_{it}'\beta + e_{it}$$

::: {.fragment}

However, there are a number of issues: <!--(most of these apply even in the friendly setting with two periods)-->

[Issue 1:]{.alert} Issues related to multiple periods and variation in treatment timing still arise

:::

::: {.fragment}


[Issue 2:]{.alert} Hard to allow parallel trends to depend on time-invariant covariates

:::

::: {.fragment}


[Issue 3:]{.alert} Hard to allow for covariates that could be affected by the treatment

:::

::: {.fragment}

[Issues 4 & 5:]{.alert} (harder to see) Can perform poorly for including time-varying covariates in the parallel trends assumption

:::


## Limitations of TWFE Regressions

Focusing on the case with two periods, to estimate the model, we take first-differences to eliminate the unit fixed effects and ultimately estimate the regression

$$\Delta Y_{it} = \Delta \theta_t + \alpha D_{it} + \Delta X_{it}'\beta + \Delta e_{it}$$

. . .

Building on work about interpreting cross-sectional regressions $Y_i = \alpha D_i + X_i'\beta + e_i$, in the presence of treatment effect heterogeneity

* @angrist-1998, @aronow-samii-2016, @sloczynski-2022, @chattopadhyay-zubizarreta-2023, @blandhol-bonney-mogstad-torgovitsky-2022, @hahn-2023, among others

we can provide a useful decomposition of $\alpha$ $\rightarrow$

## Limitations of TWFE Regressions

Can show that the coefficient $\alpha$ in the TWFE regression can be decomposed as

$$ \small \alpha = \underbrace{\E\Big[ w_1(\Delta X) ATT(X_{t=2},X_{t=1},Z)\Big| G=1 \Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[ w_1(\Delta X) \Big( \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] - \L_0(\Delta Y | \Delta X) \Big) \Big| G=1 \Big]}_{\textrm{misspecification bias}}$$

. . .

where
$$ w_1(\Delta X) := \frac{\big(1-\L(D|\Delta X)\big) \pi}{\E\big[(D-\L(D|\Delta X))^2\big]}$$

[Comments:]{.alert}

* It is possible for both weights to be negative, given that linear probability models can predict probabilities outside of the $[0,1]$ interval

* These weights are easy to estimate as they only depend on linear projections

## Limitations of TWFE Regressions {visibility="uncounted"}

Can show that the coefficient $\alpha$ in the TWFE regression can be decomposed as

$$ \small \alpha = \underbrace{\E\Big[ w_1(\Delta X) ATT(X_{t=2},X_{t=1},Z)\Big| G=1 \Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[ w_1(\Delta X) \Big( \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] - \L_0(\Delta Y | \Delta X) \Big) \Big| G=1 \Big]}_{\textrm{misspecification bias}}$$

where
$$ w_1(\Delta X) := \frac{\big(1-\L(D|\Delta X)\big) \pi}{\E\big[(D-\L(D|\Delta X))^2\big]}$$

[About the first term:]{.alert}

Ideally, we would like $w_1(\Delta X)=1$, which would imply that this term is equal to $ATT$.

Relative to this baseline, these weights have some drawbacks:

* The weights can be negative

* The weights suffer from a form of [weight reversal]{.alert-blue} (e.g., @sloczynski-2022):


## Limitations of TWFE Regressions

$$ \small \alpha = \underbrace{\E\Big[ w_1(\Delta X) ATT(X_{t=2},X_{t=1},Z)\Big| G=1 \Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[ w_1(\Delta X) \Big( \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] - \L_0(\Delta Y | \Delta X) \Big) \Big| G=1 \Big]}_{\textrm{misspecification bias}}$$

The misspecification bias component is equal to 0 if either:

1. $\E[\Delta Y|X_{t=2},X_{t=1},Z,G=0] = \L_0(\Delta Y| \Delta X)$ (i.e., the model for untreated potential outcomes is linear in $\Delta X$)

. . .

2. The implicit regression weights, $w_1(\Delta X)$, are covariate balancing weights

    * in the sense that they make the distribution of $(X_{t=2}, X_{t=1}, Z)$ to be the same for the treated untreated groups

. . .

Versions of these conditions seem plausible in the case with cross sectional data, but do not seem reasonable in the panel data context $\rightarrow$

*****

## Limitations of TWFE Regressions

Consider the condition

$$ \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] = \L_0(\Delta Y | \Delta X) $$

Notice that this condition involves two things:

a. A condition about linearity (makes sense...and similar to the cross-sectional case)

b. Changing the covariates that show up from $(X_{t=2}, X_{t=1}, Z)$ to $\Delta X$

. . .

Condition (b) amounts to changing the identification strategy from one where parallel trends only depends on $\Delta X$ rather than on $X_{t=1}$, $X_{t=2}$ and $Z$.

* In the minimum wage example, we originally wanted to compare counties with the same population and in the same region of the country.

* Condition (b) would (effectively) change this to comparing counties with similar population changes over time

* This could end up being much different from what we were originally aiming for

## Limitations of TWFE Regressions

Next, a property of implicit regression weights is that they balance the means of regressors included in the model (@chattopadhyay-zubizarreta-2023)

* This is good property in the cross-sectional setting and suggests that typically misspecification bias is likely to be small in that case

* In our case, this means that the TWFE regression will balance the mean of $\Delta X$ across groups

. . .

[More importantly:]{.alert} The TWFE does not necessarily balance variables that do not show up in the estimating equation, including:

* Levels of time-varying covariates: $X_{t=2}, X_{t=1}$

* Time-invariant covariates: $Z$

. . .

Taken together, the arguments above suggest (to me) that misspecification bias is likely to be a much bigger issue in the TWFE setting than in the cross-sectional setting

## Limitations of TWFE Regressions

In @caetano-callaway-2023, we refer to the misspecification bias term above as [hidden linearity bias]{.alert}.

. . .

What we mean is that the implications of a linear model may be much more severe in a panel data setting than in the cross-sectional setting:

* The arguments that would lead us to think that misspecification bias is typically small in cross-sectional settings do not apply for TWFE regressions

* TWFE effectively changes the identification to one where the only covariates that show up in the parallel trends assumption are $\Delta X$

## Limitations of TWFE Regressions

[What is going wrong with the TWFE regression?]{.alert}

$$Y_{it} = \theta_t + \eta_i + \alpha D_{it} + X_{it}'\beta + e_{it}$$

The source of the issues with the TWFE regression is that, when we difference out the unit fixed effect, we also transform the covariates.

* We have focused on the case with two periods and estimation in first differences, but similar issues apply in cases with more periods and with other transformations (e.g., within transformation)

. . .

[The "inherited" transformation of the covariates makes it where we are highly dependent on the model be correctly specified for this to make sense]{.alert-blue}

. . .

Instead, a better option will be to difference the outcomes (in line with parallel trends) but then to directly include the covariates that we want $(X_{t=2}, X_{t=1}, Z)$.

## Limitations of TWFE Regressions

[One last question: How much does this matter in practice?]{.alert}

* Not all that easy to check how far away $\E[\Delta Y | X_{t=1}, X_{t=2}, Z, G=0]$ is from $\L_0(\Delta Y|\Delta X)$

. . .

* Instead, an easier idea is to apply implicit regression weights to $(X_{t=2},X_{t=1},Z)$, and check if they balance these across groups

    * This gives us a way to diagnose the sensitivity of the TWFE regression to hidden linearity bias.
    * This is easy to check in practice: weights just depend on linear projections that are easy to directly estimate
    * If these are close to being balanced, it suggests that misspecification bias is small.
    * If not, then it matters a lot whether or not TWFE regression is correctly specified.

    One of the main things we will do in the application is to see how well implicit TWFE regression weights balance levels of time-varying covariates and omitted time-invariant covariates

<!--
## Cross-Sectional Regressions under Unconfoundedness

In order to understand issues 4 & 5, let us start with a simpler case where we have cross-sectional data, assume unconfoundedness

* i.e., comparisons of outcomes treated/untreated units with the same characteristics $X$ deliver, on average, causal effects

## Cross-Sectional Regressions under Unconfoundedness

[Some notation:]{.alert}

* Propensity score: $p(X) = \P(G=1|X)$

* Probability of treatment: $\pi = \P(G=1)$

. . .

[Notation for linear projections:]{.alert}

1. [Linear Projection of $Y$ on $X$ using the untreated group only]{.alert-blue}
   $$\L_0(Y|X) = X'\beta_0 \quad \text{where} \quad \beta_0 = \mathbb{E}[XX'|G=0]^{-1} \mathbb{E}[XY|G=0]$$
   * Intuition: Population version of regressing $Y$ on $X$ using only the untreated group, and
     $$\L_0(Y|X) = \E[Y|X, G=0] \quad \text{(if conditional expectation is linear).}$$


## Cross-Sectional Regressions under Unconfoundedness {visibility="uncounted"}

[Some notation:]{.alert}

* Propensity score: $p(X) = \P(G=1|X)$

* Probability of treatment: $\pi = \P(G=1)$


[Notation for linear projections:]{.alert}

2. [Linear Projection of $D$ on $X$]{.alert-blue}
   $$\L(D|X) = X'\gamma \quad \text{where} \quad \gamma = \E[XX']^{-1} \E[XD] $$
   * Intuition: Population version of regressing $D$ on $X$, and
   $$ \L(D|X) = p(X) \quad \text{(if the propensity score is linear)} $$.


::: {.notes}

* $\L_0(Y|X)$, $\L_1(Y|X)$, and $\L(D|X)$ represent **population analogs** of regressions.
* The linear projection provides predicted values based on the specified model, with correctness contingent on assumptions.

:::

## Cross-Sectional Regressions under Unconfoundedness

Consider the cross-sectional regression
$$
  Y_i = \alpha D_i + X_i'\beta + e_i
$$
where we assume uncoundedness: $Y(0) \independent D | X$

. . .

[View 1:]{.alert} Correctly specified model for $\E[Y|X,D]$.

. . .

&nbsp; In this case, $\alpha = ATT \implies$ just run the regression...

. . .

&nbsp; However, if this model is correctly specified, then it would be the case that, for any $X$

$$ \E[Y|X,G=1] - \E[Y|X,G=0] = \alpha $$

* i.e., treatment effects don't systematically vary with $X$.

* This is the sort of auxiliary restriction on [treatment effect heterogeneity]{.alert} that most modern empirical work wishes to avoid

* see, e.g., @chaisemartin-ciccia-dhaultfoeuille-knau-2024 for related linearity tests in the context of TWFE

::: {.notes}

* similar to the limitation of TWFE with staggered treatment adoption and no covariates, you can see this model as the researcher as asking the regression to do two things

    * implement the identification strategy, i.e., control for $X$
    * aggregate/summarize the effects of the treatment into a single number that is easy to report

    * it is hard to these at the same time or in one-shot, and doing them separately can often work better in practice (or at least be more robust)

:::

## Cross-Sectional Regressions under Unconfoundedness {visibility="uncounted"}

Consider the cross-sectional regression
$$
  Y_i = \alpha D_i + X_i'\beta + e_i
$$
where we assume uncoundedness: $Y(0) \independent D | X$

[View 2:]{.alert} Linear model as approximation to possibly more complicated conditional expectation

* This view allows for systematic treatment effect heterogeneity, i.e., $ATT(X)$ can vary across $X$.

* But what exactly is $\alpha$ in this case?

* There has been a lot of interesting work for this case:

    * @angrist-1998, @aronow-samii-2016, @sloczynski-2022, @chattopadhyay-zubizarreta-2023, @blandhol-bonney-mogstad-torgovitsky-2022, @hahn-2023, among others

* You can show some interesting/useful results in this case &nbsp; $\rightarrow$


## Cross-Sectional Regressions under Unconfoundedness

[Result 1: $\alpha$ can be re-interpreted as a weighting estimator]{.alert}

$$ \alpha = \E\Big[w_1(X) Y \Big| G=1\Big] - \E\Big[w_0(X) Y \Big| G=0\Big] $$

where
$$ w_1(X) := \frac{\big(1-\L(D|X)\big) \pi}{\E\big[(D-\L(D|X))^2\big]}~~~~\textrm{and}~~~~w_0(X) := \frac{\L(D|X)(1-\pi)}{\E\big[(D-\L(D|X))^2\big]} $$

and the result here follows (basically) immediately using FWL/partialling out arguments.

. . .

[Comments:]{.alert-blue}

* It is possible for both weights to be negative, given that linear probability models can predict probabilities outside of the $[0,1]$ interval

* These weights are easy to estimate as they only depend on linear projections

## Cross-Sectional Regressions under Unconfoundedness

Momentarily, it will be interesting whether or not these are covariate balancing weights in the sense of whether or not, for any function of the covariates $g$,

$$ \E\Big[ w_1(X) g(X) \Big| G=1\Big] = \E\Big[ w_0(X) g(X) \Big| G=0 \Big]$$

. . .

It turns out that, by construction, the regression weights balance the mean of $X$.  That is,
$$ \E\Big[ w_1(X) X \Big| G=1\Big] = \E\Big[ w_0(X) X \Big| G=0 \Big] $$

. . .

But they do not necessarily balance other functions of the covariates such as quadratic terms, interactions, etc.  You can check for balance by computing terms like
$$ \E\Big[ w_1(X) X^2 \Big| G=1\Big] \overset{?}{=} \E\Big[ w_0(X) X^2 \Big| G=0 \Big] $$



## Cross-Sectional Regressions under Unconfoundedness {#interpreting-regressions}

[Result 2: $\alpha$ is equal to a weighted average of $ATT(X)$ plus misspecification bias terms:]{.alert}

$$ \alpha = \underbrace{\E\Big[w_1(X) ATT(X) \Big| G=1\Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[w_1(X)\Big(\E[Y|X,G=0] - \L_0(Y|X)\Big) \Big| G=1\Big]}_{\textrm{misspecification bias}} $$

. . .

The misspecification bias component is equal to 0 if either:

1. $\E[Y|X,G=0] = \L_0(Y|X)$ (model for untreated potential outcomes is linear in $X$)

. . .

2. The implicit regression weights are covariate balancing weights

    * An example where this happens is when $p(X) = \L(D|X)$, i.e., under linearity of the propensity score

    * This occurs by constructions when all covariates are discrete and $X$ is fully saturated in these discrete covariates (e.g., @angrist-1998)

::: {.notes}

* we'll return to the weighted average term later, and about whether or not is in some sense "good"

* talk about when it would be small...(i) natural model for the outcome, (ii) the weights balance the mean of $X$, suggests they are not so bad at balancing the distribution of $X$ across groups (i'll give you some evidence about this later)

* let's put in Tymon's result here, and talk about that the weights can be negative.

:::

## Cross-Sectional Regressions under Unconfoundedness

Even if neither of the two conditions above hold, there are good reasons to think that the [misspecification bias term is likely to be small]{.alert} in most applications:

. . .

1. It's possible that $\E[Y|X,D=0]$ is not linear, but the leading choice for the model is linear

. . .

2. As dicsussed above, the implicit regression weights $w_1(X)$ and $w_0(X)$ balance the mean of $X$ across groups:

    * This doesn't mean that they balance the entire distribution of $X$ (which is what we need)
    * But, like condition 1, it does mean that the regression does something "in the ballpark" of what we'd like it to do


## Cross-Sectional Regressions under Unconfoundedness

Let us return to the first term in the expression for $\alpha$:

$$ \alpha = \underbrace{\E\Big[w_1(X) ATT(X) \Big| G=1\Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[w_1(X)\Big(\E[Y|X,G=0] - \L_0(Y|X)\Big) \Big| G=1\Big]}_{\textrm{misspecification bias}} $$

. . .

Recall that: $w_1(X) = \frac{\big(1-\L(D|X)\big) \pi}{\E\big[(D-\L(D|X))^2\big]}$

. . .

Ideally, we would like $w_1(X)=1$ for all $X$, which would imply that this term is equal to $ATT$.

[Relative to this baseline, these weights have some drawbacks:]{.alert-blue}

* The weights can be negative for some values of $X$

* The weights suffer from [weight reversal]{.alert} (e.g., @sloczynski-2022):
  * Too much weight on $ATT(X)$ for values of the covariates that are relatively uncommon for the treated group
  * Too little weight on $ATT(X)$ for values of the covariates that are relatively common for the treated group

## Limitations of TWFE Regressions

Let us now return to the TWFE regression:
$$Y_{it} = \theta_t + \eta_i + \alpha D_{it} + X_{it}'\beta + e_{it}$$
and specialize to the case with two time periods, so that we ultimately run the regression

$$\Delta Y_{it} = \Delta \theta_t + \alpha D_{it} + \Delta X_{it}'\beta + \Delta e_{it}$$


. . .

I'll highlight similar issues as in the cross-sectional case above, but I'll argue that the same sorts of issues are [likely to lead to much worse problems in the DiD setting]{.alert}

## Limitations of TWFE Regressions

Using the same arguments as above, you can show that:
$$ \small \alpha = \underbrace{\E\Big[ w_1(\Delta X) ATT(X_{t=2},X_{t=1},Z)\Big| G=1 \Big]}_{\textrm{weighted avg. of $ATT(X)$}} + \underbrace{\E\Big[ w_1(\Delta X) \Big( \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] - \L_0(\Delta Y | \Delta X) \Big) \Big| G=1 \Big]}_{\textrm{misspecification bias}}$$

. . .

Similar to above, $\alpha$ is equal to weighted averages of:

* conditional-on-covariates $ATT$'s

* misspecification bias

## Limitations of TWFE Regressions

The misspecification bias term is equal to 0 if either:

1. $\E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] = \L_0(\Delta Y | \Delta X)$.

2. The implicit regression weights $w_1(\Delta X)$ and $w_0(\Delta X)$ are covariate balancing weights that balance the distribution of $(X_{t=2}, X_{t=1}, Z)$ for the treated group relative to the untreated group.

Let's think about each of these conditions in more detail

## Limitations of TWFE Regressions

Consider the condition

$$ \E[\Delta Y | X_{t=2}, X_{t=1}, Z, G=0] = \L_0(\Delta Y | \Delta X) $$

Notice that this condition involves two things:

a. A condition about linearity (makes sense...and similar to the cross-sectional case)

b. Changing the covariates that show up from $(X_{t=2}, X_{t=1}, Z)$ to $\Delta X$

. . .

Condition (b) amounts to changing the identification strategy from one where parallel trends only depends on $\Delta X$ rather than on $X_{t=1}$, $X_{t=2}$ and $Z$.

* In the minimum wage example, we originally wanted to compare counties with the same population and in the same region of the country.

* Condition (b) would (effectively) change this to comparing counties with similar population changes over time

* This could end up being much different from what we were originally aiming for

## Limitations of TWFE Regressions

Next, the implicit regression weights $w_1(\Delta X)$ and $w_0(\Delta X)$ will balance the mean of $\Delta X$ across groups

* This is just like the cross-sectional case

* It holds because these are the regressors that show up in the estimating equation

. . .

Like the cross-sectional case, this doesn't mean that they balance the entire distribution of $\Delta X$

. . .

[More importantly:]{.alert} The TWFE does not necessarily balance the means of variables that do not show up in the estimating equation, including:

* Levels of time-varying covariates: $X_{t=2}, X_{t=1}$

* Time-invariant covariates: $Z$

. . .

Taken together, the arguments above suggest (to me) that misspecification bias is likely to be a much bigger issue in the TWFE setting than in the cross-sectional setting

## Limitations of TWFE Regressions

In @caetano-callaway-2023, we refer to the misspecification bias term above as [hidden linearity bias]{.alert}.

. . .

What we mean is that the implications of a linear model may be much more severe in a panel data setting than in the cross-sectional setting:

* The arguments that would lead us to think that misspecification bias is typically small in cross-sectional settings do not apply for TWFE regressions

* TWFE effectively changes the identification to one where the only covariates that show up in the parallel trends assumption are $\Delta X$

## Limitations of TWFE Regressions

[What is going wrong with the TWFE regression?]{.alert}

$$Y_{it} = \theta_t + \eta_i + \alpha D_{it} + X_{it}'\beta + e_{it}$$

The source of the issues with the TWFE regression is that, when we difference out the unit fixed effect, we also transform the covariates.

* We have focused on the case with two periods and estimation in first differences, but similar issues apply in cases with more periods and with other transformations (e.g., within transformation)

. . .

[The "inherited" transformation of the covariates makes it where we are highly dependent on the model be correctly specified for this to make sense]{.alert-blue}

. . .

Instead, a better option will be to difference the outcomes (in line with parallel trends) but then to directly include the covariates that we want $(X_{t=2}, X_{t=1}, Z)$.

## Limitations of TWFE Regressions

[One last question: How much does this matter in practice?]{.alert}

* Not all that easy to check how far away $\E[\Delta Y | X_{t=1}, X_{t=2}, Z, G=0]$ is from $\L_0(\Delta Y|\Delta X)$

. . .

* Instead, an easier idea is to apply implicit regression weights to $Z$, $X_{t=1}$, i.e., check:

    $$ \begin{aligned}
      \E\left[w_1(\Delta X) X_{t=1} \middle| G=1 \right] & \overset{?}{=} \E\left[w_0(\Delta X) X_{t=1} \middle| G=0 \right] \\
      \E\left[w_1(\Delta X) Z \middle| G=1 \right] & \overset{?}{=} \E\left[w_0(\Delta X) Z \middle| G=0 \right]
      \end{aligned}
    $$

    which gives us a way to diagnose the sensitivity of the TWFE regression to hidden linearity bias.
    * This is easy to check in practice: weights just depend on linear projections that are easy to directly estimate
    * If these are close, it suggests that misspecification bias is small.
    * If not, then it matters a lot whether or not the model is correctly specified.

    One of the main things we will do in the application is to see how well implicit TWFE regression weights balance levels of time-varying covariates and omitted time-invariant covariates

-->

# Part 3: Alternative Estimation Strategies {visibility="uncounted"}

## Alternative Estimation Strategies

Given the limitations of TWFE regressions, we will consider alternative estimation strategies:

1. Regression Adjustment (RA)

2. Propensity Score Weighting (IPW)

3. Augmented-Inverse Propensity Score Weighting (AIPW)

. . .

We will motivate these approaches from the two types of identification results that we showed earlier

These will have a number of better properties than TWFE regressions

## Regression Adjustment (RA)

<!--The challenging term to deal with in the previous expression for $ATT$ is-->

Recall our first identification result above:

$$ATT = \E[\Delta Y | G=1] - \E\Big[ \underbrace{\E[\Delta Y(0) | X, G=0]}_{=:m_0(X)} \Big| G=1\Big]$$

The most direct way to proceed is by proposing a model for $m_0(X)$.  For example, $m_0(X) = X'\beta_0$.

. . .

* Notice that linearity of untreated potential outcomes is exactly the same condition we needed for the TWFE regression to be a weighted average of conditional-on-covariates $ATT$'s.

. . .

* However, here $X$ includes $(X_{t=2}, X_{t=1}, Z)$ rather than only $\Delta X$.
  - This means that there could still be misspecification bias, but there is no hidden linearity bias

. . .

* Moreover, if linearity holds, we directly target $ATT$, rather than recovering a hard-to-interpret weighted average of $ATT(X)$.

. . .

* $\implies$ there is a strong case to go with RA as a default option over TWFE.

## Regression Adjustment (RA) {#ra visibility="uncounted"}

Recall our first identification result above:

$$ATT = \E[\Delta Y | G=1] - \E\Big[ \underbrace{\E[\Delta Y(0) | X, G=0]}_{=:m_0(X)} \Big| G=1\Big]$$


This expression suggests a [regression adjustment]{.alert} estimator, based on:

$$ATT = \E[\Delta Y | G=1] - \E[X'\beta_0|G=1]$$

. . .

and we can estimate the $ATT$ by

* Step 1: Estimate $\beta_0$ using untreated group

* Step 2: Compute predicted change in untreated potential outcomes for treated units: $\widehat{\Delta Y_i(0)} = X_i'\hat{\beta}_0$

* Step 3: Compute $\widehat{ATT} = \displaystyle \frac{1}{n_1} \sum_{i=1}^n G_i \big(\Delta Y_i - X_i'\hat{\beta}_0\big)$

[[Side-Discussion: One-shot imputation estimators](#side-discussion-imputation-estimators)]


## Inverse Propensity Score Weighting (IPW)

Alternatively, recall our identification strategy based on re-weighting:
$$ ATT = \E[\Delta Y | G=1] - \E[ \nu_0(X) \Delta Y | G=0] $$

. . .

The most common balancing weights are based on the propensity score, you can show:
\begin{align*}
  \nu_0(X) = \frac{p(X)(1-\pi)}{(1-p(X))\pi}
\end{align*}
where $p(X) = \P(G=1|X)$ and $\pi=\P(G=1)$.

* This is the approach suggested in @abadie-2005.  In practice, you need to estimate the propensity score.  The most common choices are probit or logit.

. . .

For estimation:

* Step 1: Estimate the propensity score (typically logit or probit)

* Step 2: Compute the weights, using the estimated propensity score

* Step 3: Compute $\widehat{ATT} = \displaystyle \frac{1}{n_1} \sum_{i=1}^n G_i \Delta Y_i - \frac{1}{n_0} \sum_{i=1}^n \frac{\hat{p}(X_i) (1-G_i)}{\big(1-\hat{p}(X_i)\big) \hat{\pi}} \Delta Y_i$

## Augmented-Inverse Propensity Score Weighting (AIPW) {#doubly-robust}

You can show an additional identification result:
<!--$$ATT=\E\left[ \left( \frac{D}{p} - \frac{p(X)(1-D)}{(1-p(X))p} \right)(\Delta Y_{t} - \E[\Delta Y_{t} | X, G=0]) \right]$$-->

$$ATT = \E\left[  \Delta Y_{t} - \E[\Delta Y_{t} | X, G=0] \big| G=1\right] - \E\left[ \frac{p(X)(1-\pi)}{(1-p(X))\pi} \big(\Delta Y_{t} - \E[\Delta Y_{t} | X, G=0]\big) \Big| G=0\right]$$


This requires estimating both $p(X)$ and $\E[\Delta Y|X,G=0]$.

. . .

[Big advantage:]{.alert}  The sample analogue of this expression $ATT$ is [doubly robust]{.alert-blue}.  This means that, it will deliver consistent estimates of $ATT$ if either the model for $p(X)$ or for $\E[\Delta Y|X,G=0]$ is correctly specified.

. . .

- In my experience, doubly robust estimators perform much better than either the regression or propensity score weighting estimators

. . .

- This also provides a connection to estimating $ATT$ under conditional parallel trends using machine learning for $p(X)$ and $\E[\Delta Y|X,G=0]$ (see: @chang-2020)


::: {.fragment}

[[more details](#understanding-double-robustness)]

:::

## Discussion

Regarding the previous issues with TWFE regressions, RA, IPW, and AIPW satisfy:

[Issue 1:]{.alert} Multiple periods <span style="color:green">&#10004;</span>

[Issue 2:]{.alert} Time-invariant covariates <span style="color:green">&#10004;</span>

[Issue 3:]{.alert} Covariates affected by the treatment
<span style="color: blue; font-weight: bold;">?</span>

[Issue 4:]{.alert} Hidden linearity bias <span style="color:green">&#10004;</span>

[Issue 5:]{.alert} Weighted average of $ATT(X)$ <span style="color:green">&#10004;</span>

. . .

You can also show that they will, by construction, balance the means of $(X_{t=2},X_{t=1},Z)$ across groups.

. . .

In my view, these are much better properties that the TWFE regression when it comes to including covariates.


# Part 4: Multiple Periods and Variation in Treatment Timing {visibility="uncounted"}

## Review of Results under Unconditional Parallel Trends

With multiple periods and variation in treatment timing, and when parallel trends holds across all groups and time periods, we previously showed that:

$$ ATT(g,t) = \E[Y_t - Y_{g-1} | G=g] - \E[Y_t - Y_{g-1} | U=1] $$

. . .

Then, if desired, we can aggregate these into $ATT^{es}(e)$ or $ATT^o$.

* This strategy amounted to a two-part strategy:

    1. Break the problem into a series of $2 \times 2$ comparisons

    2. Aggregate $ATT(g,t)$ into desired target parameter

. . .

We will follow a similar strategy here, just accounting for covariates in the parallel trends assumption

## Multiple Time Periods and Variation in Treatment Timing

<br>

::: {.callout-note}
### Conditional Parallel Trends with Multiple Periods

For all groups $g \in \bar{\mathcal{G}}$ (all groups except the never-treated group) and for all time periods $t=2, \ldots, T$,

$$\E[\Delta Y_{t}(0) | \mathbf{X}, Z, G=g] = \E[\Delta Y_{t}(0) | \mathbf{X}, Z, U=1]$$

where $\mathbf{X}_i := (X_{i1},X_{i2},\ldots,X_{iT})$.
:::

Under this assumption, using similar arguments to the ones above, one can show that

$$ATT(g,t) = \E\left[ \left( \frac{\indicator{G=g}}{\pi_g} - \frac{p_g(\mathbf{X},Z)U}{(1-p_g(\mathbf{X},Z))\pi_g}\right)\Big(Y_{t} - Y_{g-1} - m_{gt}^0(\mathbf{X},Z)\Big) \right]$$

where $p_g(\mathbf{X},Z) := \P(G=g|\mathbf{X},Z,\indicator{G=g}+U=1)$ and $m_{gt}^0(\mathbf{X},Z) := \E[Y_{t}-Y_{g-1}|\mathbf{X},Z,U=1]$.


## Practical Considerations

Because $\mathbf{X}_i$ contains $X_{it}$ for all time periods, terms like $m_{gt}^0(\mathbf{X},Z)$ can be quite high-dimensional (and hard to estimate) in many applications.

. . .

In many cases, it may be reasonable to replace with lower dimensional function $\mathbf{X}_i$:

. . .

* $\bar{X}_i$ &mdash; the average of $X_{it}$ across time periods
* $X_{it}, X_{ig-1}$ &mdash; the covariates in the current period and base period (this is possible in the `pte` package currently and may be added to `did` soon).
* $X_{ig-1}$ &mdash; the covariates in the base period (this is the default in `did`)
* $(X_{it}-X_{ig-1})$ &mdash; the change in covariates over time


. . .

Otherwise, however, everything is the same as before:

1. Recover $ATT(g,t)$

2. If desired: aggregate into $ATT^{es}(e)$ or $ATT^o$.

# Part 5: Empirical Example {visibility="uncounted"}

## Empirical Example: Minimum Wages and Employment

* This is the same application that we considered in Session 1

. . .

* Use county-level data from 2003-2007 during a period where the federal minimum wage was flat

. . .

* Exploit minimum wage changes across states

    - Any state that increases their minimum wage above the federal minimum wage will be considered as treated

. . .

* Interested in the effect of the minimum wage on teen employment

. . .

* We'll also make a number of simplifications:
  * not worry much about issues like clustered standard errors
  * not worry about variation in the amount of the minimum wage change (or whether it keeps changing) across states

. . .

[Goals: ]{.alert}

* Include covariates in the parallel trends assumption, assess how much this matters

* Try to assess how well different estimation strategies do in terms of handling covariates

## Empirical Example: Minimum Wages and Employment

Let's start by assuming that parallel trends holds conditional on a county's population and average income (sometimes we'll add region too)

* i.e., we would like to compare treated and untreated counties with similar populations and average incomes

. . .

I'll show results for the following cases:

1. Results without covariates (as a reminder of results from last time)

2. Two period TWFE with covariates

3. All periods TWFE with covariates

4. Callaway and Sant'Anna (2021) including $X_{g-1}$ and $Z$ as covariates

    * RA, IPW, AIPW

. . .

In addition to estimates, we'll also assess how well each of these works in terms of balancing covariates using the `twfeweights` package.

<!--    * with and without region as a covariate

4. One-shot imputation where the model is $Y_{it}(0) = \theta_t + \eta_i + X_{it}'\beta + e_{it}$

-->


```{r}
#| echo: false
library(did)
library(BMisc)
library(twfeweights)
library(fixest)
library(modelsummary)
library(ggplot2)
load("data2.RData")
data2$region <- droplevels(data2$region)
```

## TWFE Results without Covariates

```{r}
twfe_res2 <- fixest::feols(lemp ~ post | id + year,
                           data=data2,
                           cluster="id")
```

<br>

```{r}
modelsummary(list(twfe_res2), gof_omit=".*")
```


## Event Study without Covariates (Callaway and Sant'Anna) {visibility="uncounted"}

```{r warning=FALSE}
attgt <- did::att_gt(yname="lemp",
                     idname="id",
                     gname="G",
                     tname="year",
                     data=data2,
                     control_group="nevertreated",
                     base_period="universal")
attes <- aggte(attgt, type="dynamic")
ggdid(attes)
```


## Overall ATT without Covariates (Callaway and Sant'Anna) {visibility="uncounted"}

```{r}
attO <- did::aggte(attgt, type="group")
summary(attO)
```


## Two periods TWFE Regression with Covariates

```{r}
# run TWFE regression
data2_subset <- subset(data2, year %in% c(2003,2004))
data2_subset <- subset(data2_subset, G %in% c(0, 2004))
twfe_x <- fixest::feols(lemp ~ post + lpop + lavg_pay | id + year,
                        data=data2_subset,
                        cluster="id")
modelsummary(twfe_x, gof_omit=".*")
```

## Diagnose covariate balance {visibility="uncounted"}

```{r}
library(twfeweights)
tp_wts <- two_period_reg_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~lpop + lavg_pay,
  extra_balance_vars_formula = ~region,
  data = data2_subset
)
```

## Diagnose covariate balance {visibility="uncounted"}

```{r}
ggtwfeweights(tp_wts,
              absolute_value=FALSE,
              standardize=TRUE,
              plot_relative_to_target=FALSE) +
  xlim(c(-2,2))
```

## TWFE with more periods and covariates

```{r}
# run TWFE regression
twfe_x <- fixest::feols(lemp ~ post + lpop + lavg_pay | id + year,
                        data=data2,
                        cluster="id")
modelsummary(twfe_x, gof_omit=".*")
```

## Diagnose covariate balance {visibility="uncounted"}

```{r}
#| cache: true
twfe_wts <- implicit_twfe_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~lpop + lavg_pay,
  data = data2,
  base_period = "gmin1"
)
covariate_balance <- twfe_cov_bal(twfe_wts, ~ region + lpop + lavg_pay + -1)
```

## Diagnose covariate balance {visibility="uncounted"}

```{r}
ggtwfeweights(covariate_balance,
              absolute_value = FALSE,
              standardize = TRUE,
              plot_relative_to_target = FALSE) +
  xlim(c(-1,1))
```


## CS (2021) AIPW, $X_{g-1}, Z$

```{r eval=FALSE}
#| code-line-numbers: "|9"
# callaway and sant'anna including covariates
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="dr",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```


## CS (2021) AIPW, $X_{g-1}, Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="dr",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
```

## CS (2021) AIPW, $X_{g-1}, Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```

## Check covariate balance {visibility="uncounted"}

```{r}
# similar code as before...check course materials
```


```{r}
#| echo: false
#| cache: true
aipw_wts <- implicit_aipw_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~ region + lpop + lavg_pay,
  d_covs_formula = ~ 1,
  data = data2
)

aipw_cov_bal <- aipw_cov_bal(aipw_wts, ~ region + lpop + lavg_pay + -1)
ggtwfeweights(aipw_cov_bal, absolute_value = FALSE,
              standardize = TRUE,
              plot_relative_to_target = FALSE) +
  xlim(c(-1,1))
```

## Additional Results and Bonus Material {#application-end}

[Additional Results:]{.alert}

* Add region as a covariate in TWFE [[details](#results-add-region-as-a-covariate)]

* One-shot imputation estimators [[details](#results-one-shot-imputation-estimators)]

* Regression adjustment [[details](#results-regression-adjustment)]

* Inverse propensity score weighting [[details](#results-inverse-propensity-score-weighting)]

[Bonus Material:]{.alert}

* "Bad Controls" [[details](#bad-controls)]

## Conclusion

* Including covariates in the parallel trends assumption can make difference-in-differences identification strategies more plausible

* If you want to include covariates in the parallel trends assumption, it is better to use approaches that directly include the covariates relative to estimation strategies that transform the covariates

* In the minimum wage application, we did better in terms of covariate balance with regression adjustment and AIPW; however, there still seemed to be violations of parallel trends in pre-treatment periods

    * I know some ways to get rid of these apparent violations of parallel trends, but I want to use this as an angle to get invited back for Session 3...



# Appendix {visibility="uncounted"}

## Side-Discussion: Imputation Estimators {#side-discussion-imputation-estimators visibility="uncounted"}

Regression adjustment is closely related to imputation estimators, which we talked about in the first session.

. . .

In settings with multiple periods and variation in treatment timing, these are often operationalized in different ways though

* In @callaway-santanna-2021, we considered regression adjustment at the group-time level $\implies$ do $2 \times 2$ regression adjustment many times and then aggreggate

* However, most imputation estimators are implemented in [one-shot]{.alert}, where you would typically estimate the model
$$Y_{it}(0) = \theta_t + \eta_i + X_{it}'\beta + e_{it}$$
    across all time periods

## Side-Discussion: Imputation Estimators {visibility="uncounted"}

One-shot imputation estimators have similar limitations as TWFE regressions when it comes to covariates:

$$Y_{it}(0) = \theta_t + \eta_i + X_{it}'\beta + e_{it}$$

[Issue 1:]{.alert} Multiple periods <span style="color:green">&#10004;</span>

[Issue 2:]{.alert} Time-invariant covariates <span style="color:orange;font-weight:bold">&#9888;</span>

[Issue 3:]{.alert} Covariates affected by the treatment &#10060;

[Issue 4:]{.alert} Hidden linearity bias &#10060;

* You can see that it implicitly relies on $\E[\Delta Y(0) | X_{t=2}, X_{t=1}, Z, G=0] = \E[\Delta Y(0) | \Delta X, G=0]$

[Issue 5:]{.alert} Weighted average of $ATT(X)$ <span style="color:green">&#10004;</span>

[[back](#ra)]

## Understanding Double Robustness {#understanding-double-robustness visibility="uncounted"}

To understand double robustness, we can rewrite the expression for $ATT$ as
\begin{align*}
  ATT = \E\left[ \frac{D}{\pi} \Big(\Delta Y - m_0(X)\Big) \right] - \E\left[ \frac{p(X)(1-D)}{(1-p(X))\pi} \Big(\Delta Y - m_0(X)\Big)\right]
\end{align*}

The first term is exactly the same as what comes from regression adjustment

* If we correctly specify a model for $m_0(X)$, it will be equal to $ATT$.

* If $m_0(X)$ not correctly specified, then, by itself, this term will be biased for $ATT$

The second term can be thought of as a de-biasing term

* If $m_0(X)$ is correctly specified, it is equal to 0

* If $p(X)$ is correctly specified, it reduces to $\E[\Delta Y_{t}(0) | G=1] - \E[m_0(X)|G=1]$ which both delivers counterfactual untreated potential outcomes and removes the (possibly misspecified) second term from the first equation

[[Back](#doubly-robust)]

## Add region as a covariate {#results-add-region-as-a-covariate visibility="uncounted"}

We'll allow for path of outcomes to depend on region of the country

```{r}
# run TWFE regression
twfe_x <- fixest::feols(lemp ~ post + lpop + lavg_pay | id + region^year,
                        data=data2,
                        cluster="id")
modelsummary(twfe_x, gof_omit=".*")
```

Relative to previous results, this is much smaller---this is (broadly) in line with the literature where controlling for `region` often matters a great deal (e.g., @dube-lester-reich-2010).

## Check covariate balance {visibility="uncounted"}

```{r}
# similar code as before...check course materials
```

```{r}
#| echo: false
#| cache: true
twfe_wts2 <- implicit_twfe_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~lpop + lavg_pay + as.factor(region)*as.factor(year),
  data = data2,
  base_period = "gmin1"
)
covariate_balance2 <- twfe_cov_bal(twfe_wts2, ~ region + lpop + lavg_pay + -1)

ggtwfeweights(covariate_balance2,
              absolute_value = FALSE,
              standardize = TRUE,
              plot_relative_to_target = FALSE) +
  xlim(c(-1,1))
```

[[back](#application-end)]

## One-shot imputation (i.e., regression adjustment with only $\Delta X$ as covariate) {#results-one-shot-imputation-estimators visibility="uncounted"}

```{r}
#| cache: true
# it's reg. adj. even though the function says aipw...
ra_wts <- implicit_aipw_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~ 1,
  d_covs_formula = ~ lpop + lavg_pay,
  pscore_formula = ~1,
  data = data2
)
ra_wts$est
```

i.e., we estimate a somewhat larger effect of the minimum wage on teen employment

## One-shot imputation (i.e., regression adjustment with only $\Delta X$ as covariate){visibility="uncounted"}

```{r}
ra_cov_bal <- aipw_cov_bal(ra_wts, ~ region + lpop + lavg_pay + -1)
ggtwfeweights(ra_cov_bal, absolute_value = FALSE,
              standardize = TRUE,
              plot_relative_to_target = FALSE) +
  xlim(c(-1,1))
```

[[back](#application-end)]


## CS (2021) Regression Adjustment, $X_{g-1}, Z$ {#results-regression-adjustment visibility="uncounted"}

```{r eval=FALSE}
#| code-line-numbers: "|6|9"
# callaway and sant'anna including covariates
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="reg",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```


## CS (2021) Regression Adjustment, $X_{g-1},Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="reg",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
```

## CS (2021) Regression Adjustment, $X_{g-1},Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```

## Check covariate balance {visibility="uncounted"}

```{r}
# similar code as before...check course materials
```


```{r}
#| echo: false
#| cache: true
ra_wts <- implicit_aipw_weights(
  yname = "lemp",
  tname = "year",
  idname = "id",
  gname = "G",
  xformula = ~ region + lpop + lavg_pay,
  d_covs_formula = ~ 1,
  pscore_formula = ~1,
  data = data2
)

ra_cov_bal <- aipw_cov_bal(ra_wts, ~ region + lpop + lavg_pay + -1)
ggtwfeweights(ra_cov_bal, absolute_value = FALSE,
              standardize = TRUE,
              plot_relative_to_target = FALSE) +
  xlim(c(-1,1))
```

[[back](#application-end)]


## CS (2021) IPW, $X_{g-1}, Z$ {#results-inverse-propensity-score-weighting visibility="uncounted"}

```{r eval=FALSE}
#| code-line-numbers: "|9"
# callaway and sant'anna including covariates
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="ipw",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```



## CS (2021) IPW, $X_{g-1}, Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x <- att_gt(yname="lemp",
               tname="year",
               idname="id",
               gname="G",
               xformla=~region + lpop + lavg_pay,
               control_group="nevertreated",
               base_period="universal",
               est_method="ipw",
               data=data2)
cs_x_res <- aggte(cs_x, type="group")
summary(cs_x_res)
```

## CS (2021) IPW, $X_{g-1}, Z$ {visibility="uncounted"}

```{r echo=FALSE}
cs_x_dyn <- aggte(cs_x, type="dynamic")
ggdid(cs_x_dyn)
```

[[back](#application-end)]

# Part 6: Covariates Affected by the Treatment {#bad-controls visibility="uncounted"}

## Covariates Affected by the Treatment {#covariates-affected-by-the-treatment visibility="uncounted"}

So far, our discussion has been for the case where the time-varying covariates [evolve exogenously]{.alert}.

* Many (probably most) covariates fit into this category: in the minimum wage example, a county's population probably fits here.

. . .

In some applications, we may want to control for covariates that themselves could be affected by the treatment

* Classical examples in labor economics: A person's industry, occupation, or union status

* These are often referred to as ["bad controls"]{.alert}

. . .

You can see a [tension]{.alert} here:

* We would like to compare units who, absent being treated, would have had the same (say) union status

* But union status could be affected by the treatment

## Covariates Affected by the Treatment {visibility="uncounted"}

The most common practice is to just completely drop these covariates from the analysis

* Not clear if this is the right idea though...

We will consider some alternatives

* Condition on pre-treatment value of bad control
* Treat bad control as an outcome (i.e., use some identification strategy), then feed this into the main analysis as a covariate


## Additional Notation {visibility="uncounted"}

To wrap our heads around this, let's go back to the case with two time periods.

Define treated and untreated [potential covariates]{.alert}: $X_{it}(1)$ and $X_{it}(0)$.  Notice that in the "textbook" two period setting, we observe
$$X_{it=2} = D_i X_{it=2}(1) + (1-D_i) X_{it=2}(0) \qquad \textrm{and} \qquad X_{it=1} = X_{it=1}(0)$$

Then, we will consider parallel trends in terms of untreated potential outcomes and untreated potential covariates:

<br>

::: {.callout-note}
### Conditional Parallel Trends using Untreated Potential Covariates

$$\E[\Delta Y(0) | X_{t=2}(0), X_{t=1}(0), Z, G=1] = \E[\Delta Y(0) | X_{t=2}(0), X_{t=1}(0), Z, G=0]$$
:::

<br>

## Identification Issues {visibility="uncounted"}

Following the same line of argument as before, it follows that

$$ATT = \E[\Delta Y | G=1] - \E\Big[ \E[\Delta Y(0) | X_{t=2}(0), X_{t=1}(0), Z, G=0] \Big| G=1\Big]$$

The second term is the tricky one.  Notice that:

* The inside conditional expectation is identified &mdash; we see untreated potential outcomes and covariates for the untreated group

* However, we cannot average over $X_{t=2}(0)$ for the treated group, because we don't observe $X_{t=2}(0)$ for the treated group

. . .

There are several options for what we can do &nbsp; $\rightarrow$

## Option 1: Ignore {visibility="uncounted"}

One idea is to just ignore that the covariates may have been affected by the treatment:

<br>

::: {.callout-note}

### Alternative Conditional Parallel Trends 1

$$\E[\Delta Y(0) | { \color{red} X_{\color{red}{t=2} } }, X_{t=1}(0), Z, G=1] = \E[\Delta Y(0) |  { \color{red} X_{\color{red}{t=2}} }, X_{t=1}(0), Z, G=0]$$
:::

<br>

The limitations of this approach are well known (even discussed in MHE), and this is not typically the approach taken in empirical work

. . .

[Job Displacement Example:]{.alert-blue} You would compare paths of outcomes for workers who left union [because they were displaced]{.alert} to paths of outcomes for non-displaced workers who also left union (e.g., [because of better non-unionized job opportunity]{.alert})

## Option 2: Drop {visibility="uncounted"}

It is more common in empirical work to drop $X{it}(0)$ entirely from the parallel trends assumption

<br>

::: {.callout-note}

### Alternative Conditional Parallel Trends 2

$$\E[\Delta Y(0) | Z, G=1] = \E[\Delta Y(0) |  Z, G=0]$$
:::

<br>

In my view, this is not attractive either though.  If we believe this assumption, then we have basically solved the bad control problem by assuming that it does not exist.

. . .

[Job Displacement Example:]{.alert-blue} We have now just assumed that path of earnings (absent job displacement) doesn't depend on union status

## Option 3: Tweak {visibility="uncounted"}

Perhaps a better alternative identifying assumption is the following one

<br>

::: {.callout-note}

### Alternative Conditional Parallel Trends 3

$$\E[\Delta Y(0) | X_{t=1}(0), Z, G=1] = \E[\Delta Y(0) |  X_{t=1}(0), Z, G=0]$$
:::

<br>

. . .

[Intuition:]{.alert} Conditional parallel trends holds after conditioning on pre-treatment time-varying covariates that could have been affected by treatment

. . .


[Job Displacement Example:]{.alert-blue} Path of earnings (absent job displacement) depends on pre-treatment union status, but not untreated potential union status in the second period

. . .

[What to do:]{.alert} Since $X_{it=1}(0)$ is observed for all units, we can immediately operationalize this assumption use our arguments from earlier (i.e., the ones without bad controls)

* This is difficult to operationalize with a TWFE regression

* In practice, you can just include the bad control among other covariates in `did`

::: {.notes}

* It is not the same identifying assumption as the one that we started with, but we are at least allowing for the bad control to show up in the identifying assumption

:::

## Option 4: Extra Assumptions {visibility="uncounted"}

Another option is to keep the original identifying assumption, but add additional assumptions where we (in some sense) treat $X_t$ as an outcome and as a covariate.

Recall:

$$ATT = \E[\Delta Y | G=1] - \E\Big[ \E[\Delta Y(0) | X_{t=2}(0), X_{t=1}(0), Z, G=0] \Big| G=1\Big]$$

. . .

If we could figure out distribution of $X_{t=2}(0)$ for the treated group, we could recover $ATT$

## Option 4: Dealing with $X_{t=2}(0)$ {visibility="uncounted"}

::: {.callout-note}

### Covariate Unconfoundedness Assumption

$$X_{t=2}(0) \independent D | X_{t=1}(0), Z$$

:::

[Intuition:]{.alert} For the treated group, the time-varying covariate would have evolved in the same way over time as it actually did for the untreated group, conditional on $X_{t=1}$ and $Z$.

* Notice that this assumption only concerns untreated potential covariates $\implies$ it allows for $X_{t=2}$ to be affected by the treatment

* Making an assumption like this indicates that $X_{t=2}(0)$ is playing a dual role: (i) start by treating it as if it's an outcome, (ii) have it continue to play a role as a covariate

Under this assumption, can show that we can recover the $ATT$:

$$ATT = \E[\Delta Y | G=1] - \E\left[ \E[\Delta Y | X_{t=1}, Z, G=0] \Big| G=1 \right]$$

This is the same expression as in Option 3

## Option 4: Additional Discussion {visibility="uncounted"}

In some cases, it may make sense to condition on other additional variables (e.g., the lagged outcome $Y_{t=1}$) in the covariate unconfoundedness assumption.  In this case, it is still possible to identify $ATT$, but it is more complicated

It could also be possible to use alternative identifying assumptions besides covariate unconfoundedness &mdash; at a high-level, we somehow need to recover the distribution of $X_{t=2}(0)$

* e.g., @brown-butts-westerlund-2023

See @caetano-callaway-payne-santanna-2022 for more details about bad controls.

[[Back](#conclusion)]



# References {visibility="uncounted"}

::: {#refs}
:::